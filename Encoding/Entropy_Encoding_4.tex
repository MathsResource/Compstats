\documentclass[a4]{beamer}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{newlfont}
\usepackage{amsmath,amsthm,amsfonts}
%\usepackage{beamerthemesplit}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{mathptmx}  % Font Family
\usepackage{helvet}   % Font Family
\usepackage{color}

\mode<presentation> {
 \usetheme{Default} % was Frankfurt
 \useinnertheme{rounded}
 \useoutertheme{infolines}
 \usefonttheme{serif}
 %\usecolortheme{wolverine}
% \usecolortheme{rose}
\usefonttheme{structurebold}
}

\setbeamercovered{dynamic}

\title[MA4413t]{Statistics for Computing \\ {\normalsize Lecture 10B}}
\author[Kevin O'Brien]{Kevin O'Brien \\ {\scriptsize kevin.obrien@ul.ie}}
\date{Summer 2011}
\institute[Maths \& Stats]{Dept. of Mathematics \& Statistics, \\ University \textit{of} Limerick}

\renewcommand{\arraystretch}{1.5}


\begin{document}
%-----------------------------------------------------------------------------------------------------------------------------------------------------------%
\medskip
\noindent \textbf{ ENTROPY CODING}
The design of a variable-length code such that its average code word length approaches the
entropy of the DMS is often referred to as enlmpy coding. In this section we present two examples of
entropy coding.
\begin{itemize}
	\item Shannon- Fano Coiding
	\item Huffman Coding
\end{itemize}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------%
\medskip
% Page 255
\frame{A. Shannon-Fun Coding:}
An efficient code can be obtained by the following simple procedure, known as
Shannon- Fano algorithm:
\begin{itemize}
\item[1.] List the source symbols in order of decreasing probability.
\item[2.] Partition the set into two sets that are as close to equiprobable as possible, and assign 0 to the
upper set and 1 to the lower set.
\end{itemize}


%-----------------------------------------------------------------------------------------------------------------------------------------------------------%
% Page 256 Bottom
\medskip\noindent \textbf{B. Huffman Encoding:}
In general, Huffman encoding results in an optimum code. Thus, it is the code that has the highest
efliciency.\\ The Huffman encoding procedure is as follows:
\begin{itemize}\item[1.] List the source symbols in order of decreasing probability.
\item[2.] Combine the probabilities of the two symbols having the lowest probabilities, and reorder
the resultant probabilities; this step is called reduction 1. The same procedure is repeated until
there are two ordered probabilities remaining.
\item[3.] Start encoding with the last reduction, which consists of exactly two ordered probabilities. Assign
0 as the first digit in the code words for all the source symbols associated with the first probability;
assign 1 to the second probability.
\item[4.] Now go back and assign 0 and 1 to the second digit for the two probabilities that were combined
in the previous reduction step, retaining all assignments made in Step 3.
\item[5.] Keep regressing this way until the first column is reached.
\end{itemize}


%An example of Huffman encoding is shown in Table 10-3.
%H(X) = 2.36b/symbol
%L = 2.38 b/symbol
%\nu = 0.99


%-----------------------------------------------------------------------------------------------------------------------------------------------------------%
\medskip
10.5. A high-resolution black—and-white TV picture consists of about $2 \times 10^6$  picture elements and 16
different brightness levels. Pictures are repeated at a rate of 32 per second. All picture elements
are assumed to be independent, and all levels have equal likelihood of occurrence. Calculate the
average rate of information conveyed by this TV picture source.\\ \bigskip

%Hm — -§ img L — 4 11/111e111e111
%P, 16 Z is
%1 : 2(l0§)(32) : 64(l0°) elements/s
%Hence, by Eq. (10.10)
%R = rH(X) : 64(l0°)(4) = 256(l0°) b/s Z 256 Mb/s





\medskip
\noindent \textbf{Channel Capacity}

\textbf{A. Channel Capacity per Symbol C:}\\
The channel cizpucily per symbol of a DMC is detined as
\[ 
C. = \mbox{max}I(X; Y) \qquad \mbox{ b/symbol }
\]
where the maxiniization is over all possible input probability distributions {P(x,)} on X. Note that the
channel capacity CA is a function of only the channel transition probabilities that define the channel.

%---------------------------------------------------------------------------------------------------------------------------------------------------%

\medskip
\noindent \textbf{Channel Capacity}
\textbf{B. Channel Capacity per Second C:}\\
If r symbols are being transmitted per second, then the maximum rate of transmission of
information per second is rC>.. This is the channel capacity per secvml and is denoted by C (bls):
C : rC, b/s (10.34)

%---------------------------------------------------------------------------------------------------------------------------------------------------%
\medskip
\noindent \textbf{C. Capacities of Special Channels:}
\textbf{ 1. Lossless Channel:}\\
For a lossless channel, H(X|Y) Z 0 % (Prob. 10.10) and '
$I(X; Y):1—1(X) $
Thus, the mutual information (information transfer) is equal to the input (source) entropy, and no
source infomation is lost in transmission. Consequently. the channel capacity per symbol is
\[
Cs : max H(X) = logzm %(10.36)
\]
where m is the number of symbols in X.


%---------------------------------------------------------------------------------------------------------------------------------------------------%
\medskip
2. Deterministic Channel:
For a deterministic channel, H( Y|X) = 0 for all input distributions Pm), and
I(X; Y) : H(Y) (10.37)
Thus. the information transfer is equal to the output cntropy. The channel capacity per symbol is

C. : max H(Y) = 1og2 yi (10.38)
iron)
where ri is the number of symbols in Y.


%---------------------------------------------------------------------------------------------------------------------------------------------------%
\medskip
\textbf{3. Naireless Channel:}\\
Since a noiseless channel is both lossless and deterministic, we have
I(X; Y) : H(X) = H(Y) (l0.39)
and the channel capacity per symbol is
C, : logzm :1ogZ n (10440)
\textbf{4. Binary Symmetric Channel:}\\
For the BSC o1`Fig. 10-5. the mutual infomation is (Prob. 10.16)
1(X: Y) ; 11(Y)-l-p1¤g;1¤+(1·1>)l<>2;;(l—p) (1041)
and the channel capacity per symbol is
Ci = 1 +¤1¤>z21¤+(l —p)l<>g2(1 ~p) (10.42)



%---------------------------------------------------------------------------------------------------------------------------------------------------%













\medskip
The unit of I(x)) is the bit (binary unit) if b Z 2, Hartley or decit if b = 10, and nat (natural unir) if
b Z at It is standard to use b Z 2. Here the unit bit (abbreviated "b“) is a measure ofinlbrrnation
content and is not to bc confused with the term hit meaning "binary digit." The conversion of these
units to other units can be achieved by the following relationships.

\[   \]

%-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------%
%  page
\medskip
\noindent \textbf{Average Information or Entropy}
\begin{itemize}
\item In a practical communication system, we usually transmit long sequences of symbols from an
information source. \item Thus, we are more interested in the average infornation that a source produces
than the information content of a single symbol.
\item The mean value of $ l(x_i)$ over the alphabet of source X with tn different symbols is given by
\[ \]
\end{itemize}

%\end{document}

%-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------%


% page 247 

\medskip
\noindent \textbf{Information Rate}
If the time rate at which source X emits symbols is r (symbols/s), the lnformation rate R of the
source is given by

%\[R ; r11(X) b/s (10.10)\]








%---------------------------------------------------------------------------------------------------------------------------------------%
\medskip
\noindent \textbf{ MUTUAL INFORMATION}
\textbf{A. Conditional and Joint Entropies:}\\
Using the input probabilities P(x,), output probabilities $P(y_i)$, transition probabilities P(yJ|>r,),
and joint probabilities P(x,, yy), we can define the following various entropy functions for a channel
with m inputs and n outputs:
 
\begin{itemize}
\item H(X) = - X P(xi) log %; P(x;) (10.21)
\item H(Y) = -2P(yj)]%<>gz Pty;) UU-22)
\item H<X I Y) = - X X %P<>¤r,yy)1<¤gz Ptmlyj) <10.23)
\item H = -2 ZP%(>¢..y,) 1022 P(y,|X.) (10-24)
\item H(X, Y) - -2 Z F%<><..y,)1¤zz P(><r.y;) <10·25)
\end{itemize}


%---------------------------------------------------------------------------------------------------------------------------------------%
\medskip
\noindent \textbf{Conditional and Joint Entropy}
These entropies can be interpreted as follows: H(X) is the average uncertainty of the channel input,
and H(Y) is the average uncertainty of the channel output. The conditional entropy H(X]Y) is a
measure of the average uncertainty remaining about the channel input after the channel output has
been observed. And H(X] Y) is sometimes called the equivncation of X with respect to Y. \begin{itemize} \item The
conditional entropy H(Y|X) is the average uncertainty of the channel output given that X was
transmitted.\item  The joint entropy H(X, Y) is the average uncertainty of the communication channel as a
whole.\end{itemize}

%-----------------------------------------------------------------------------------------------%
\medskip
Two useful relationships among the above various entropies are
\begin{itemize} \item
$H(X, Y)=H(X|Y)+H(Y) (10,26)$
$H(X,Y)=H(Y|X)+H(X) (10.27)$
\end{itemize}
B. Mutual Information:
The mutual information 1(X; Y) of a channel is deiined by
I(X; Y) = H(X)— H(X|Y) b/symbol (10.28)


%-----------------------------------------------------------------------------------------------%
\medskip % ULCIS 
\noindent \textbf{Self Information}Self-information
This is defined by the following mathematical formula:$I(A) = −logb P(A)$

The self-information of an event measures the amount of one’s surprise
evoked by the event. The negative logarithm $−logb P(A)$ can also be written as \[
log_b  {1 \over P(A)} \]
Note that log(1) = 0, and that $| − log(P(A))|$ increases as P(A) decreases
from 1 to 0. This supports our intuition from daily experience. For example,
a low-probability event tends to cause more ``surprise".

%---------------------------------------------------------------------------------------------------------------------------------------%



\medskip
\noindent \textbf{Code efficiency and Code redundancy}
% Pg 253/254
The parameter $L$ represents the average number of bits per source symbol used in the source coding process.
The code efficiency is defined as \[\nu = {L_{min} \over L} \]where $L_{min}$ is the minimum possiblve value of $L$. When $\nu$ approaches unity, the codes is said to be efficient. 
The code redundancy $\gamma$ is defined as $\gamma = 1- \nu$.



%---------------------------------------------------------------------------------------------------------------------------------------%
\medskip
%page 254
\noindent \textbf{Source Coding Theorem}
The source coding theorem states that for zi DMS X with entropy $H(X)$, the average code word length $L$ per symbol is bounded as
L 2 H(X) (10.52)

and further, L can bc made as close to H(X) as dcsircd for some suitably chosen code.
Thus, with$ L_{min} \geq H(X)$.

The code efficiency can be rewritten as
\[\nu = {H(X) \over L} \]


---------------------------------------------------------------------------%
\medskip
\noindent \textbf{ Kraft inequality}
\begin{itemize}
\item Let X be a DMS with alphabet ($x _i = \{1, 2, . . . ,m\}$). Assume that the length of the assigned binary
code word corresponding to x, is n,.
\item A necessary and sufficient condition for the existence of an instantaneous binary code is
 
 \[ K = \sum^{m}_{i=1}2^{-n_i} \leq 1 \]
which is known as the \textbf{Kraft inequality}.
\item Note that the Kraft inequality assures us of the existence of an instantaneously decodable code
with code word lengths that satisfy the inequality. But it does not show us how to obtain these code
words, nor does it say that any code that satisfies the inequality is automatically uniquely decodable
\end{itemize}




\end{document}
