
\noindent \textbf{Source Coding}
\begin{itemize}
\item A conversion of the output of a DMS into a sequence of binary symbols (binary code word) is
called \textbf{\emph{source coding}}.
\item  The device that performs this conversion is called the source encoder.
\item
An objective of source coding is to minimize the average bit rate required for representation of the
source by reducing the redundancy of the information source.
\end{itemize}



\noindent \textbf{Source Coding : Code Length and Code Efficiency}
\begin{itemize}

\item
Let X be a DMS with finite entropy $H(X)$ and an alphabet $\{x_1 , \ldots,  x_m\}$, each with corresponding
probabilities of occurrence $P(x_i)$. \item Let the binary code word assigned to symbol $x_i$ by
the encoder have length $n_i$ b. \item The length of a code word is the number of binary digits
in the code word. The average code word length L, per source symbol is given by

\[ E(L) = \sum ^{m}_{i=1} P(x_i) n_i \]
\end{itemize}




\noindent \textbf{Source Coding : Code efficiency and Code redundancy}
% Pg 253/254
\begin{itemize}
\item The parameter $L$ (estimated by $E(L)$) represents the average number of bits per source symbol used in the source coding process.\item
The code efficiency is defined as \[\eta = {L_{min} \over L} \]where $L_{min}$ is the minimum possible value of $L$. When $\eta$ approaches unity, the codes is said to be efficient.
\item The code redundancy $\gamma$ is defined as $\gamma = 1- \eta$.
\end{itemize}



%---------------------------------------------------------------------------------------------------------------------------------------%

%page 254
\noindent \textbf{Source Coding Theorem}
\begin{itemize}
\item The source coding theorem states that for a DMS X with entropy $H(X)$, the average code word length $L$ per symbol is bounded as $L \geq H(X)$

\item Furthermore L can be made as close to H(X) as required for some suitably chosen code.
\item Thus, with $ L_{min} \geq H(X)$, the code efficiency can be rewritten as
\[\eta = {H(X) \over L} \]
\item We will use this definition for efficiency. (Remark $L$ is estimable by $E(L)$.)
\end{itemize}






%-----------------------------------------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{ ENTROPY CODING}
The design of a variable-length code such that its average code word length approaches the
entropy of the DMS is often referred to as enlmpy coding. In this section we present two examples of
entropy coding.
\begin{itemize}
\item Shannon- Fano Coiding
\item Huffman Coding
\end{itemize}

%-----------------------------------------------------------------------------------------------------------------------------------------------------------%

% Page 255
{A. Shannon-Fun Coding:}
An efficient code can be obtained by the following simple procedure, known as
Shannon- Fano algorithm:
\begin{itemize}
\item[1.] List the source symbols in order of decreasing probability.
\item[2.] Partition the set into two sets that are as close to equiprobable as possible, and assign 0 to the
upper set and 1 to the lower set.
\end{itemize}


%-----------------------------------------------------------------------------------------------------------------------------------------------------------%
% Page 256 Bottom
\noindent \textbf{B. Huffman Encoding:}
In general, Huffman encoding results in an optimum code. Thus, it is the code that has the highest
efliciency.\\ The Huffman encoding procedure is as follows:
\begin{itemize}\item[1.] List the source symbols in order of decreasing probability.
\item[2.] Combine the probabilities of the two symbols having the lowest probabilities, and reorder
the resultant probabilities; this step is called reduction 1. The same procedure is repeated until
there are two ordered probabilities remaining.
\item[3.] Start encoding with the last reduction, which consists of exactly two ordered probabilities. Assign
0 as the first digit in the code words for all the source symbols associated with the first probability;
assign 1 to the second probability.
\item[4.] Now go back and assign 0 and 1 to the second digit for the two probabilities that were combined
in the previous reduction step, retaining all assignments made in Step 3.
\item[5.] Keep regressing this way until the first column is reached.
\end{itemize}


%An example of Huffman encoding is shown in Table 10-3.
%H(X) = 2.36b/symbol
%L = 2.38 b/symbol
%\nu = 0.99

%----------------------------------------------------------------------------------%

{
Huffman coding is an entropy encoding algorithm used for lossless data compression.


}
{

\noindent \textbf{Ambiguity}

Ambiguity occurs if there is any path to some symbol whose label is a prefix of the label of a path to some other symbol. In the Huffman tree, every symbol is a \textbf{\emph{leaf}}. Thus it is impossible for the label of a path to a leaf to be a prefix of any other path label, and so the mapping defined by Huffman coding has an inverse and decoding is possible.
}


\noindent \textbf{Example}
For a simple example, we will take a short phrase and derive our probabilities from a frequency count of letters within that phrase. The resulting encoding should be good for compressing this phrase, but of course will be inappropriate for other phrases with a different letter distribution.

"All you base are belong to us"
}

{
\noindent \textbf{What is Information?}
\begin{itemize} \item Once we agree to define the information of an event ain terms of P(a), the properties (2) and (3) will be satisfied if the information in ais defined as
\[ I(a) = -log P(a)\]

\item Remark : The base of the logarithm depends on the unit of information to be used.
\end{itemize}
}



