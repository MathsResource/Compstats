\documentclass[a4]{beamer}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{newlfont}
\usepackage{amsmath,amsthm,amsfonts}
%\usepackage{beamerthemesplit}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps,pgfshade}
\usepackage{mathptmx}  % Font Family
\usepackage{helvet}   % Font Family
\usepackage{color}

\mode<presentation> {
 \usetheme{Default} % was Frankfurt
 \useinnertheme{rounded}
 \useoutertheme{infolines}
 \usefonttheme{serif}
 %\usecolortheme{wolverine}
% \usecolortheme{rose}
\usefonttheme{structurebold}
}

\setbeamercovered{dynamic}

\title[MA4413]{Statistics for Computing \\ {\normalsize MA4413 Lecture 8B}}
\author[Kevin O'Brien]{Kevin O'Brien \\ {\scriptsize Kevin.obrien@ul.ie}}
\date{Autumn Semester 2013}
\institute[Maths \& Stats]{Dept. of Mathematics \& Statistics, \\ University \textit{of} Limerick}

\renewcommand{\arraystretch}{1.5}

%------------------------------------------------------------------------%
\begin{document}
%--------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Hypothesis Testing}
\large
Recall: the inferential step to conclude that the null hypothesis is false goes as follows: The data (or data more extreme) are very unlikely given that the null hypothesis is true.
\bigskip
This means that:
\begin{itemize}\item [(1)] a very unlikely event occurred or
\item[(2)] the null hypothesis is false. \end{itemize}
The inference usually made is that the null hypothesis is false. Importantly it doesnt prove the null hypothesis to be false.

%-------------------------------------------------------------------------------------------------------------------------%

\noindent \textbf{Type I and II errors}
\large
There are two kinds of errors that can be made in hypothesis testing:
\begin{itemize}
\item[(1)] a true null hypothesis can be incorrectly rejected
\item[(2)] a false null hypothesis can fail to be rejected.
\end{itemize}
The former error is called a \textbf{\emph{Type I error}} and the latter error is called a \textbf{\emph{Type II error}}. \\ \bigskip
The probability of Type I error is always equal to the level of significance $\alpha$ (alpha) that is used as the standard for rejecting the null hypothesis .

%---------------------------------------------------------------------------%

\noindent \textbf{Type II Error}
\begin{itemize}

\item The probability of a Type II error is designated by the Greek letter beta ($\beta$).
\item A Type II error is only an error in the sense that an opportunity to reject the null hypothesis correctly was lost.
\item It is not an error in the sense that an incorrect conclusion was drawn since no conclusion is drawn when the null hypothesis is not rejected.
\end{itemize}

%---------------------------------------------------------------------------%

\noindent \textbf{Types of Error}
\large
\begin{itemize}
\item
A Type I error, on the other hand, is an error in every sense of the word. A conclusion is drawn that the null hypothesis is false when, in fact, it is true. \item Therefore, Type I errors are generally considered more serious than Type II errors.
\item
The probability of a Type I error ($\alpha$ ) is set by the experimenter. \item There is a trade-off between Type I and Type II errors. The more an experimenter protects himself or herself against Type I errors by choosing a low level, the greater the chance of a Type II error.
\end{itemize}

%---------------------------------------------------------------------------%

\noindent \textbf{Types of Error}
\large
\begin{itemize}
\item
Requiring very strong evidence to reject the null hypothesis makes it very unlikely that a true null hypothesis will be rejected. \item However, it increases the chance that a false null hypothesis will not be rejected, thus increasing the likelihood of Type II error.
\item
The Type I error rate is almost always set at 0.05 or at 0.01, the latter being more conservative since it requires stronger evidence to reject the null hypothesis at the 0.01 level then at the 0.05 level.
\item \textbf{Important}In this module, the significance level $\alpha$ can be assumed to be 0.05, unless explicitly stated otherwise.
\end{itemize}

%---------------------------------------------------------------------------%

\noindent \textbf{Type I and II errors}
\large
These two types of errors are defined in the table below.
\small
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
&True State: H0 True & True State: H0 False\\\hline
Decision: Reject H0 & Type I error& Correct\\\hline
Decision: Do not Reject H0 & Correct &Type II error\\ \hline
\end{tabular}
\end{center}



\noindent \textbf{p-values}
\begin{itemize}
\item In hypothesis tests, the difference between the observed value and the parameter value specified by $H_0$ is computed and the probability of obtaining a difference this large or large is calculated.
\item The probability of obtaining data as extreme, or more extreme, than the expected value under the null hypothesis is called the \textbf{\emph{p-value}}.
\item It is not the probability of the null hypothesis itself.
\item Suppose if the probability value is $0.0175$, this does not mean that the probability that the null hypothesis is either true (or false) is $0.0175$.

\end{itemize}

%--------------------------------------------------------------------------------------%

\noindent \textbf{p-values}
\begin{itemize}
\item the p-value means that the probability of obtaining data as different or more different from the null hypothesis as those obtained in the experiment is $0.0175$.
\item If the p-value is less than the specified significance level, adjusted for the number of tails, then we reject the null hypothesis.
\[\mbox{ is p-value} \leq \frac{\alpha}{k} \mbox{?}\]
\end{itemize}



\noindent \textbf{The Hypothesis Testing Procedure }
The second procedures is very similar to the first, but is more practicable for written exams, so we will use this one more. The first two steps are the same.

\begin{itemize}
\item Formally write out the null and alternative hypotheses (already described).
\item Compute the test statistic
\item Determine the \emph{\textbf{critical value}} (described shortly)
\item Make a decision based on the critical value.
\end{itemize}




\noindent \textbf{Hypothesis Tests for single samples}

\begin{itemize}
\item We could have inference procedures for single sample studies. We would base an argument on the either the sample mean or sample proportion as appropriate.
\item A hypothesis test can be used to determine how ``confident" we can be with our data in making that statements.
\item The lower the significance level (The margin for Type I error) the stronger our data must be.
\item Large samples lead to more confident conclusion.
\end{itemize}


\noindent \textbf{Hypothesis Tests for single samples}
\begin{itemize}
\item We could have either hypothesis test for the sample mean or the sample proportion, to test a statement about the population as a whole (i.e something about the population mean)
\item We make our argument in the form of the null and alternative hypotheses. 
\item The Hypothesis testing procedure determines the strength of evidence in making our arguments. 
\end{itemize}


\noindent \textbf{Hypothesis Tests for single samples}
\begin{itemize}
\item We simply follow the four step procedure. 
\item All of the components are the same used in confidence intervals.
\item The critical value is simply a quantile from the $Z$ or $t-$distribution.
\item The standard errors are also as before. Although when performing a hypothesis test for proportions, we use the expected value under the null hypothesis, rather than point estimate. (reason beyond scope of course.)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5



%%%% Type I and Type II errors here
{
\noindent \textbf{The Paired t-test}
A paired t-test is used to compare two population means where you have two samples in
which observations in one sample can be \textbf{\emph{paired}} with observations in the other sample.\\
\bigskip
Examples of where this might occur are:
\begin{itemize}
\item Before-and-after observations on the same subjects (e.g. students diagnostic test
results before and after a particular module or course).
\item A comparison of two different methods of measurement or two different treatments
where the measurements/treatments are applied to the \textbf{\emph{same}} subjects.
\end{itemize}
The difference between two paired measurements is known as a \textbf{\emph{case-wise}} difference.
}



%-------------------------------------------------------------------------------------------%

\noindent \textbf{The Paired t-test}
\begin{itemize}
\item We will often be required to compute the case-wise differences, the average of those differences and the standard deviation of those difference.

\item The mean difference for a set of differences between paired observations is
\[ \bar{d} = {\sum d_i \over n }\]

\item The computational formula for the standard deviation of the differences
between paired observations is
\[s_d = \sqrt{ {\sum d_i^2 - n\bar{d}^2 \over n-1}}\]
\item It is nearly always a small sample test.
\end{itemize}



%----------------------------------------------------------------------------------------------------%
{
\noindent \textbf{The Paired t-test}
\begin{itemize}
\item $\mu_d$ mean value for the population of case-wise differences.
\item The null hypothesis is that that $\mu_d = 0$
\item Given $\bar{d}$ mean value for the sample of differences, and $s_d$ standard deviation of the differences for the paired sample data, we can compute this test in the same manner as a one-sample test for the mean
\end{itemize}
}


\end{document}
