
\item[(c)] \textbf{\textit{Communication Channels (14 Marks)}}\\
The input source to a noisy communication channel is a random variable X over the
four symbols $\{a, b, c, d\}$. The output from this channel is a random variable Y over these same
four symbols. \\
\noindent 
The joint distribution of these two random variables is as follows:\\ 
\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline
y=a & 0.125	&	0.03125	&	0	&	0.015625	\\ \hline
y=b & 0	&	0.1875	&	0.125	&	0	\\ \hline
y=c & 0	&	0.015625	&	0.1875	&	0	\\ \hline
y=d & 0.0625	&	0	&	0	&	0.25	\\ \hline
\end{tabular}
\end{center}

\begin{itemize}
\item[(i)] (2 Marks) Write down the marginal distribution for $X$ and compute the marginal entropy $H(X)$.
\item[(ii)] (2 Marks) Write down the marginal distribution for $Y$ and compute the marginal entropy $H(Y )$.
\item[(iii)] (2 Marks) What is the joint entropy $H(X, Y ) $ of the two random variables?
\item[(iv)] (4 marks) What is the conditional entropy $H(Y|X)$?
\item[(v)] (2 marks) What is the conditional entropy $H(X|Y)$?
\item[(vi)] (2 marks) What is the mutual information $I(X;Y)$ between the two random variables?
\end{itemize}
\end{itemize}